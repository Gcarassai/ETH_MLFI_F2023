{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ryBjUDEFqBz"
   },
   "source": [
    "**Please write the names of all group members here:** Giulio Vittorio Carassai, SÃ¶ren Lambrecht, Alihan Kerestecioglu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Note:* The provided structure for the code below is only suggestive, and if you want to structure your programs differently you may do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKy-4MNYF-Do"
   },
   "source": [
    "Exercise 1. (Write down the general equation for $V$ depending on the given parameters. You may use the Black-Scholes formulas without proof.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first define for each underlying asset j = 1, ..., 4 the following quantities:\n",
    "\n",
    "$$ d_1^j = \\frac{\\log (S_0^j/K) + (r + \\sigma^2/2)T}{\\sigma \\sqrt{t}} $$\n",
    "\n",
    "$$ d_2^j = d_1^j - \\sigma \\sqrt{T} $$\n",
    "\n",
    "Applying Black-Scholes Call Option Price we then define for j = 1,2 (in the following we use the notation where $\\mathcal{N}(\\cdot)$ is the CDF of a std gaussian):\n",
    "\n",
    "$$ V_j = S_0^j \\mathcal{N}(d_1^j) - K e^{-rT} \\mathcal{N}(d_2^j) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; j = 1,2$$\n",
    "\n",
    "and for j = 3,4 we define the Put Option Price similarly as:\n",
    "\n",
    "$$ V_j = K e^{-rT} N(-d_2^j) - S_0^j N(- d_1^j) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; j = 3,4$$\n",
    "\n",
    "It then follows that the whole portfolio time 0 value is \n",
    "\n",
    "$$ V = \\sum_{j=1}^4 V_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "n8ct7ilYE0Cj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 2.a)\n",
      "Sanity Check:  55.03296129776206 is almost 55\n",
      "i) Approximation Error for (100,100,100,100) =  3.2782988222492975\n",
      "ii) for n =  100\n",
      "L1 Approximation Error: 1.1446840655196078\n",
      "CI:  (0.9804158201775574, 1.308952310861658)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.a)\n",
    "# supressing warnings of deprecated versions of methods\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For all exercises in this assignment, the following packages are sufficient:\n",
    "import numpy as np\n",
    "from scipy.stats import norm  # standard normal distribution\n",
    "\n",
    "# You may use the black scholes formulas from below\n",
    "def black_scholes_call(s0, K, T, r, sigma):\n",
    "    d1 = (np.log(s0/K) + (r + sigma**2/2)*T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return s0 * norm.cdf(d1) - K * np.exp(-r*T)* norm.cdf(d2)\n",
    "\n",
    "def black_scholes_put(s0, K, T, r, sigma):\n",
    "    return black_scholes_call(s0, K, T, r, sigma) - s0 + np.exp(-T * r) * K\n",
    "\n",
    "\n",
    "# Implement the function V\n",
    "# As a sanity check:  For the given parameters, the value for\n",
    "#                     x = (100, 100, 100, 100) should be around 55\n",
    "def V(x,K=100,T=1,r=0.02, sigmas=[0.1*(1+i) for i in range(1,5)]):\n",
    "    # call values\n",
    "    value_calls = 0\n",
    "    for i in range(0,2):\n",
    "        value_calls += black_scholes_call(x[:,i], K, T, r, sigmas[i])\n",
    "    # put values\n",
    "    value_puts = 0\n",
    "    for i in range(2,4):\n",
    "        value_puts += black_scholes_put(x[:,i], K, T, r, sigmas[i])\n",
    "    #return sum of all values\n",
    "    return value_calls + value_puts\n",
    "\n",
    "print(\"Exercise 2.a)\")\n",
    "print(\"Sanity Check: \", V(np.asarray([[100, 100, 100, 100]]))[0], \"is almost 55\")\n",
    "\n",
    "# Simulate (x_1, ..., x_1000) and (y_1, ..., y_1000)\n",
    "np.random.seed(0)  # fix the random seed so the results are reproducible\n",
    "m = 1000\n",
    "x_training = np.random.uniform(low=80., high=120., size=[m, 4])\n",
    "y = V(x_training)  # or similar\n",
    "\n",
    "# Calculate the matrix A\n",
    "A = np.concatenate((np.ones((m,1)), x_training), axis=1)\n",
    "\n",
    "# Solve the normal equation; you may for instance use np.linalg.solve\n",
    "b = np.linalg.solve(np.transpose(A) @ A, np.transpose(A) @ y)\n",
    "\n",
    "# (i) Report the approximation error for x = (100, 100, 100, 100)\n",
    "def f_linear(x, beta = b):\n",
    "    return beta[0] + x @ beta[1:5]\n",
    "\n",
    "x = 100*np.ones((1,4))\n",
    "print(\"i) Approximation Error for (100,100,100,100) = \", (f_linear(x)-V(x))[0])\n",
    "\n",
    "# (ii) Report the L^1 approximation error.\n",
    "# Generate n test samples, calculate the confidence interval, and\n",
    "# potentially readjust the value of n if the bounds are too wide.\n",
    "n = 100\n",
    "x_test = np.random.uniform(low=80., high=120., size=[n, 4])\n",
    "\n",
    "def l1_error_sample(x_test, b):\n",
    "    return np.abs(f_linear(x_test, b) - V(x_test))\n",
    "\n",
    "l1_error_approx_sample = l1_error_sample(x_test, b)\n",
    "\n",
    "# find CI\n",
    "import scipy.stats\n",
    "\n",
    "def get_conf_int(l1_error_approx_sample):\n",
    "    m, std = np.mean(l1_error_approx_sample), np.std(l1_error_approx_sample)\n",
    "    h = std/np.sqrt(len(l1_error_approx_sample)) * scipy.stats.norm.ppf(0.975)\n",
    "    return (m-h, m+h)\n",
    "\n",
    "print(\"ii) for n = \", n)\n",
    "print(\"L1 Approximation Error:\", np.mean(l1_error_approx_sample))\n",
    "print(\"CI: \", get_conf_int(l1_error_approx_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 2.b)\n",
      "i) Approximation Error for (100,100,100,100) =  0.018710719169050094\n",
      "ii) We chose n =  3000  so that the width of the CI is less then the approximation error\n",
      "L1 Approximation Error: 0.06894113653448584\n",
      "CI:  (0.06712154944284648, 0.0707607236261252)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.b)\n",
    "# Calculate the matrix B. The function np.concatenate may be useful\n",
    "def create_B(data):\n",
    "    length = data.shape[0]\n",
    "    B = np.ones(length).reshape(length,1)\n",
    "    B = np.concatenate((B, data), axis=1)\n",
    "    for i in range(4):\n",
    "        for j in range(i,4):\n",
    "            val = data[:, j] * data[:, i]\n",
    "            B = np.concatenate((B, val.reshape(length,1)), axis = 1)\n",
    "    return B\n",
    "\n",
    "B = create_B(x_training)\n",
    "\n",
    "# Solve the normal equation\n",
    "b_poly = np.linalg.solve(B.T @ B, B.T @ y)\n",
    "b_poly\n",
    "\n",
    "def f_poly(x, beta):\n",
    "    return x @ beta\n",
    "\n",
    "# (i) Report the approximation error for x = (100, 100, 100, 100)\n",
    "x = 100*np.ones((1,4))\n",
    "x_transformed = create_B(x)\n",
    "print(\"Exercise 2.b)\")\n",
    "print(\"i) Approximation Error for (100,100,100,100) = \", (f_poly(x_transformed, b_poly) - V(x))[0])\n",
    "\n",
    "# (ii) Report the L^1 approximation error.\n",
    "# Generate n test samples, calculate the confidence interval, and\n",
    "# potentially readjust the value of n if the bounds are too wide.\n",
    "\n",
    "n = 3000\n",
    "x_test = np.random.uniform(low=80., high=120., size=[n, 4])\n",
    "\n",
    "\n",
    "x_test_transformed = create_B(x_test)\n",
    "l1_error_approx = np.abs(f_poly(x_test_transformed, b_poly) - V(x_test))\n",
    "\n",
    "#do the CI\n",
    "print(\"ii) We chose n = \", n, \" so that the width of the CI is less then the approximation error\")\n",
    "print(\"L1 Approximation Error:\", np.mean(l1_error_approx))\n",
    "print(\"CI: \", get_conf_int(l1_error_approx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "q5o-KbJo-jmN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3. a)\n",
      "Monte Carlo Estimate: 42.58197407478139\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3. a)\n",
    "# Define a general function Vmc taking as input x = s_0\n",
    "# (it helps to implement this with a variable number of Monte Carlo simulations)\n",
    "\n",
    "# set varibales apart from s0\n",
    "K = 100\n",
    "T = 1\n",
    "r = 0.02\n",
    "sigma = np.array([(j + 1)*0.1 for j in range(1,5)])\n",
    "\n",
    "def V_mc(s0, n_iterations):\n",
    "    payoff = []\n",
    "    for _ in range(n_iterations):\n",
    "        W = np.random.randn(4) # mean 0 std dev 1\n",
    "        s = s0 * np.exp((r - 0.5*sigma**2) * T + sigma*W)\n",
    "        max_s = np.max(s)\n",
    "        \n",
    "        payoff.append(np.maximum((max_s - K), 0))\n",
    "        \n",
    "    return np.exp(-r * T) * np.mean(payoff)\n",
    "\n",
    "# Evaluate Vmc at x = (100, 100, 100, 100)\n",
    "# Sanity check: While the value depends on the particular Monte-Carlo sample, it should be around 40 to 45\n",
    "\n",
    "n_iterations = 10000\n",
    "s0 = np.array([100, 100, 100, 100])\n",
    "\n",
    "print(\"Exercise 3. a)\")\n",
    "print(\"Monte Carlo Estimate:\", V_mc(s0, n_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3.b)\n",
      "Estimated parameters of function g(.) : [ 6.16544863e+02 -3.64108819e+00 -7.29507070e+00 -1.92737812e+00\n",
      " -1.53385319e-01  3.15508813e-02 -8.23239822e-03 -5.01863464e-03\n",
      " -1.33919975e-02  3.56292318e-02  5.61392416e-03  7.62763392e-03\n",
      "  7.60933558e-03  8.05530013e-03  2.77190035e-03]\n",
      "-----------------------------------------------\n",
      "Approximation error for x = (100,100,100,100):  -5.610133960613133\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.b)\n",
    "# Simulate points x_i and y_i as described.\n",
    "\n",
    "K = 100\n",
    "T = 1\n",
    "r = 0.02\n",
    "sigma = np.array([(j + 1)*0.1 for j in range(1,5)])\n",
    "\n",
    "n = 1000\n",
    "np.random.seed(0)\n",
    "x_simulated = np.random.uniform(low=80., high=120., size=[n, 4])\n",
    "# print('x_simulated shape:', x_simulated.shape)\n",
    "\n",
    "# Note that y_i are basically evaluations of Vmc using only 1 simulation to\n",
    "# compute the Monte Carlo average\n",
    "xi = np.random.randn(n, 4)\n",
    "\n",
    "# Calculate the max call option payoffs\n",
    "ST = x_simulated * np.exp(sigma * xi - 0.5 * sigma**2 * T) - K * np.exp(-r*T)\n",
    "# print('ST shape:', ST.shape)\n",
    "\n",
    "y_simulated = np.maximum(np.max(ST, axis = 1), 0)\n",
    "# print('sigma shape:', sigma.shape)\n",
    "# print('y_simulated shape:', y_simulated.shape)\n",
    "\n",
    "# Compute normal least square regression by solving the normal equation\n",
    "\n",
    "B = create_B(x_simulated)\n",
    "# Solve the normal equation\n",
    "b_poly_2 = np.linalg.solve(B.T @ B, B.T @ y_simulated)\n",
    "# print('Shape of Extended Design matrix:', B.shape)\n",
    "# print('Shape of Parameters:', b_poly_2.shape)\n",
    "\n",
    "def g(x, beta):\n",
    "    return x @ beta\n",
    "\n",
    "print(\"Exercise 3.b)\")\n",
    "print('Estimated parameters of function g(.) :', b_poly_2)  # small check\n",
    "\n",
    "#report approximation error on x = (100,100,100,100)\n",
    "s0 = np.array([100,100,100,100]).reshape(1,4)\n",
    "approximation_error = g(create_B(s0), b_poly_2)[0] - V_mc(s0, 10000)\n",
    "print('-----------------------------------------------')\n",
    "print('Approximation error for x = (100,100,100,100): ', approximation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "OpO1K0O7A_hH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3.c)\n",
      "For threshold 0.5: achieved an approximation error of 1916.1573299922443\n",
      "For threshold 1: achieved an approximation error of 1916.1573299922443\n",
      "For threshold 10: achieved an approximation error of 1916.1573299922443\n",
      "For threshold 100: achieved an approximation error of 1907.813056067831\n",
      "For threshold 1000: achieved an approximation error of 1917.7301156937642\n",
      "For threshold 10000: achieved an approximation error of 1903.67936490263\n",
      "For threshold 100000: achieved an approximation error of 1909.8049959856246\n",
      "Achieved best average CV score of 1903.67936490263, with the threshold 10000\n",
      "------------------------------------------------\n",
      "Approximation error for x = (100,100,100,100) :  4.895709754677959\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.c) \n",
    "print(\"Exercise 3.c)\")\n",
    "n = 1000\n",
    "np.random.seed(0)\n",
    "x_simulated = np.random.uniform(low=80., high=120., size=[n, 4])\n",
    "xi = np.random.randn(n, 4)\n",
    "ST = x_simulated * np.exp(sigma * xi - 0.5 * sigma**2 * T) - K * np.exp(-r*T)\n",
    "y = np.maximum(np.max(ST, axis = 1), 0) # simulated y values\n",
    "X = create_B(x_simulated) # extended design matrix\n",
    "\n",
    "# To calculate the singular value decomposition you can use np.linalg.svd\n",
    "\n",
    "# First split the data into 5 subgroups\n",
    "# (while this is inefficient for memory, for this small example here it's fine)\n",
    "\n",
    "# Define a plausible truncation threshold c > 0\n",
    "\n",
    "def get_pseudo_in(B, threshold):\n",
    "    U,S,Vh = np.linalg.svd(B, full_matrices=False)\n",
    "    S_trunc = np.where(S > threshold, 1/S, 0)\n",
    "    # Compute the truncated pseudoinverse of B\n",
    "    B_pseudo_inv = Vh.T @ np.diag(S_trunc) @ U.T\n",
    "    return B_pseudo_inv\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "#eigenvalues, _ = np.linalg.eig(np.dot(X.T, X))\n",
    "#print(np.sqrt(np.max(eigenvalues)) * 0.0001)\n",
    "thresholds = [0.5, 1, 10, 100, 1000, 10000, 100000]\n",
    "cv_scores = defaultdict(list) # format will be like {0.1: [f1_s, f2_s], 0.2: [f1_s, f2_s]}\n",
    "\n",
    "# Iterate over the subgroups (each subgroup serves as validation data once)\n",
    "# START ITERATE HERE\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    #print(f\"Fold {i}\")\n",
    "    #print(f\"  Validation:  index={val_index[0]} to {val_index[-1]}\")\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    for threshold in thresholds:\n",
    "        # fit\n",
    "        X_inv = scipy.linalg.pinv(X_train, rcond=threshold)\n",
    "        #X_inv = get_pseudo_in(X_train, threshold=threshold)\n",
    "        beta = np.dot(X_inv, y_train)\n",
    "        # predict val set\n",
    "        y_pred = X_val @ beta\n",
    "        # compute error\n",
    "        err = np.mean((y_pred - y_val)**2)  #np.mean(np.abs(y_pred - y_val))\n",
    "        cv_scores[threshold].append(err)\n",
    "\n",
    "#print('------------')       \n",
    "#print(cv_scores)\n",
    "#print('------------')        \n",
    "\n",
    "# END ITERATE HERE\n",
    "# Calculate the generalization error as the average over the different\n",
    "# approximation errors across the 5 subgroups.\n",
    "threshold_avg = {}\n",
    "for threshold, scores in cv_scores.items():\n",
    "    threshold_avg[threshold] = np.mean(scores)\n",
    "    print(f'For threshold {threshold}: achieved an approximation error of {np.mean(scores)}')\n",
    "\n",
    "# Try a few different truncation levels and choose one which achieves good\n",
    "# generalization error. You can comment for clarity all values you tried\n",
    "\n",
    "# Report the approximation error\n",
    "import operator\n",
    "best_threshold = min(threshold_avg.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'Achieved best average CV score of {threshold_avg[best_threshold]}, with the threshold {best_threshold}') \n",
    "\n",
    "#report approximation error on x = (100,100,100,100)\n",
    "s0 = np.array([100,100,100,100]).reshape(1,4)\n",
    "\n",
    "#fit model with best threshhold\n",
    "X_inv_all = scipy.linalg.pinv(X, rcond= best_threshold)\n",
    "beta_all = np.dot(X_inv_all, y)\n",
    "\n",
    "g_x =  create_B(s0) @ beta_all\n",
    "\n",
    "approximation_error = g_x - V_mc(s0, 10000)\n",
    "print('------------------------------------------------')\n",
    "print('Approximation error for x = (100,100,100,100) : ', approximation_error[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "gT02-2i3A78q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3. d)\n",
      "Achieved best average CV score of 1907.5530614113218, with Ridge(alpha=1000)\n",
      "------------------------------------------------\n",
      "Approximation error for x = (100,100,100,100) :  -2.5722855066988686\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3. d) \n",
    "print(\"Exercise 3. d)\")\n",
    "#For each of the approximating functions g obtained in b)âd), report the approximation error g(x) â VËmc(x) \n",
    "#for x = (100, 100, 100, 100), where VËmc(x) is the Monte Carlo estimate from a).\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "param_grid = {\n",
    "    'alpha'  : [0.01, 1, 10, 100, 1000, 10000, 11000]\n",
    "}\n",
    "estimator = Ridge()\n",
    "reg = GridSearchCV(estimator, param_grid, scoring= \"neg_mean_squared_error\", cv=5) #custom_scorer\n",
    "reg.fit(X, y)\n",
    "cv_scores = pd.DataFrame(reg.cv_results_)[[f'split{i}_test_score' for i in range(5)] + ['mean_test_score']]\n",
    "#print(cv_scores.head(10))\n",
    "print(f'Achieved best average CV score of {-reg.best_score_}, with {reg.best_estimator_}') \n",
    "\n",
    "#report approximation error on x = (100,100,100,100)\n",
    "s0 = np.array([100,100,100,100]).reshape(1,4)\n",
    "\n",
    "#fit model with best threshhold\n",
    "g_x = reg.predict(create_B(s0))\n",
    "\n",
    "approximation_error = g_x - V_mc(s0, 10000)\n",
    "print('------------------------------------------------')\n",
    "print('Approximation error for x = (100,100,100,100) : ', approximation_error[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "IxKhdT_aMXCE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 4 a)\n",
      "Parameters Estimated: [ 1.87507056e+02 -4.19151790e-01 -4.32126314e-01 -6.27054486e-01\n",
      " -1.39228704e+00  2.31845707e-03 -1.55180749e-03  3.00824625e-04\n",
      "  8.44416254e-04  4.25226078e-04  2.12954533e-04  5.19065521e-03\n",
      "  1.95195477e-03  1.81011660e-03  2.90728954e-03]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 a) \n",
    "print(\"Exercise 4 a)\")\n",
    "# Simulate new labels y_i^L. Note that those correspond to Vmc evaluations with\n",
    "# L many simulations for the Monte Carlo estimate\n",
    "\n",
    "K = 100\n",
    "T = 1\n",
    "r = 0.02\n",
    "sigma = np.array([(j + 1)*0.1 for j in range(1,5)])\n",
    "\n",
    "L = 1000\n",
    "n = 1000\n",
    "\n",
    "x_simulated = np.random.uniform(low=80., high=120., size=[n, 4])\n",
    "#print('x_simulated shape:', x_simulated.shape)\n",
    "\n",
    "y_simulated_reduced = np.zeros(n)\n",
    "for l in range(L):\n",
    "\n",
    "    # Note that y_i are basically evaluations of Vmc using only 1 simulation to\n",
    "    # compute the Monte Carlo average\n",
    "    xi = np.random.randn(n, 4)\n",
    "\n",
    "    # Calculate the max call option payoffs\n",
    "    ST = x_simulated * np.exp(sigma * xi - 0.5 * sigma**2 * T) - K * np.exp(-r*T)\n",
    "    #print('ST shape:', ST.shape)\n",
    "\n",
    "    y_simulated = np.maximum(np.max(ST, axis = 1), 0)\n",
    "    #print('sigma shape:', sigma.shape)\n",
    "    #print('y_simulated shape:', y_simulated.shape)\n",
    "    \n",
    "    y_simulated_reduced += y_simulated\n",
    "    \n",
    "y_simulated_reduced = y_simulated_reduced / L\n",
    "\n",
    "#print('y_simulated_reduced shape:', y_simulated_reduced.shape)\n",
    "\n",
    "# a)\n",
    "# solve the least square regression by solving the normal equation\n",
    "x_transformed = create_B(x_simulated)\n",
    "# Solve the normal equation\n",
    "b_poly_reduced = np.linalg.solve(B.T @ B, B.T @ y_simulated_reduced)\n",
    "# print('Shape of Extended Design matrix:', B.shape)\n",
    "# print('Shape of Parameters:', b_poly_reduced.shape)\n",
    "\n",
    "def g(x, beta):\n",
    "    return x @ beta\n",
    "\n",
    "print('Parameters Estimated:', b_poly_reduced)\n",
    "# print('-------------------------------------')\n",
    "# print('Average training approximation error:',np.mean((g(x_transformed, b_poly_reduced) - y_simulated_reduced )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 4b)\n",
      "For threshold 0.0001: achieved an approximation error of 61.60660033763121\n",
      "For threshold 0.001: achieved an approximation error of 61.60660033763121\n",
      "For threshold 0.01: achieved an approximation error of 61.60660033763121\n",
      "For threshold 0.05: achieved an approximation error of 61.60660033763121\n",
      "For threshold 0.1: achieved an approximation error of 61.60660033763121\n",
      "For threshold 0.15: achieved an approximation error of 53.51796047965136\n",
      "For threshold 0.25: achieved an approximation error of 53.51796047965136\n",
      "For threshold 1: achieved an approximation error of 53.51796047965136\n",
      "-----------------------------------------------\n",
      "With Pseudo Inverse, achieved best average CV score of 53.51796047965136, with the threshold 0.15\n",
      "-----------------------------------------------\n",
      "With Ridge, achieved best average CV score of 1.8078501995901164, with Ridge(alpha=100)\n",
      "With Ridge, the abs approximation error is 1.0975065882756587, \n",
      " and with linear regression \\ it is 6.2195834708630215\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4b)\n",
    "print(\"Exercise 4b)\")\n",
    "# Solve the problem using truncated pseudoinversion and/or ridge regression as\n",
    "# in Exercise 3 c) or d).\n",
    "# What are the levels of optimal regularization and truncation?\n",
    "# (How do they differ to the ones in Exercise 3?)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "#eigenvalues, _ = np.linalg.eig(np.dot(X.T, X))\n",
    "#print(np.sqrt(np.max(eigenvalues)) * 0.0001)\n",
    "thresholds = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.25, 1]\n",
    "cv_scores = defaultdict(list) # format will be like {0.1: [f1_s, f2_s], 0.2: [f1_s, f2_s]}\n",
    "\n",
    "# Iterate over the subgroups (each subgroup serves as validation data once)\n",
    "# START ITERATE HERE\n",
    "for i, (train_index, val_index) in enumerate(kf.split(x_transformed)):\n",
    "    #print(f\"Fold {i}\")\n",
    "    #print(f\"  Validation:  index={val_index[0]} to {val_index[-1]}\")\n",
    "    X_train, X_val = X[train_index], x_transformed[val_index]\n",
    "    y_train, y_val = y[train_index], y_simulated_reduced[val_index]\n",
    "    for threshold in thresholds:\n",
    "        # fit\n",
    "        X_inv = scipy.linalg.pinv(X_train, rcond=threshold)\n",
    "        #X_inv = get_pseudo_in(X_train, threshold=threshold)\n",
    "        beta = np.dot(X_inv, y_train)\n",
    "        # predict val set\n",
    "        y_pred = X_val @ beta\n",
    "        # compute error\n",
    "        err = np.mean((y_pred - y_val)**2) \n",
    "        cv_scores[threshold].append(err)\n",
    "\n",
    "#print('------------')       \n",
    "#print(cv_scores)\n",
    "#print('------------')        \n",
    "\n",
    "# END ITERATE HERE\n",
    "# Calculate the generalization error as the average over the different\n",
    "# approximation errors across the 5 subgroups.\n",
    "threshold_avg = {}\n",
    "for threshold, scores in cv_scores.items():\n",
    "    threshold_avg[threshold] = np.mean(scores)\n",
    "    print(f'For threshold {threshold}: achieved an approximation error of {np.mean(scores)}')\n",
    "\n",
    "# Try a few different truncation levels and choose one which achieves good\n",
    "# generalization error. You can comment for clarity all values you tried\n",
    "\n",
    "# Report the approximation error\n",
    "best_threshold = min(threshold_avg.items(), key=operator.itemgetter(1))[0]\n",
    "print('-----------------------------------------------')\n",
    "print(f'With Pseudo Inverse, achieved best average CV score of {threshold_avg[best_threshold]}, with the threshold {best_threshold}')\n",
    "print('-----------------------------------------------')\n",
    "\n",
    "param_grid = {\n",
    "    'alpha'  : [1, 10, 100, 1000, 1100, 1200, 10000]\n",
    "}\n",
    "estimator = Ridge()\n",
    "reg = GridSearchCV(estimator, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "reg.fit(x_transformed, y_simulated_reduced)\n",
    "cv_scores = pd.DataFrame(reg.cv_results_)[[f'split{i}_test_score' for i in range(5)] + ['mean_test_score']]\n",
    "#print(cv_scores.head(10))\n",
    "print(f'With Ridge, achieved best average CV score of {-reg.best_score_}, with {reg.best_estimator_}')\n",
    "\n",
    "#generate some test data to make a comparision\n",
    "s0 = np.random.uniform(low=80., high=120., size=[100, 4])\n",
    "\n",
    "#print('x_simulated shape:', x_simulated.shape)\n",
    "\n",
    "V_mc_reduced = np.zeros(100)\n",
    "for l in range(1000):\n",
    "\n",
    "    # Note that y_i are basically evaluations of Vmc using only 1 simulation to\n",
    "    # compute the Monte Carlo average\n",
    "    xi = np.random.randn(100, 4)\n",
    "\n",
    "    # Calculate the max call option payoffs\n",
    "    ST = s0 * np.exp(sigma * xi - 0.5 * sigma**2 * T) - K * np.exp(-r*T)\n",
    "    #print('ST shape:', ST.shape)\n",
    "\n",
    "    y_simulated = np.maximum(np.max(ST, axis = 1), 0)\n",
    "    #print('sigma shape:', sigma.shape)\n",
    "    #print('y_simulated shape:', y_simulated.shape)\n",
    "    \n",
    "    V_mc_reduced += y_simulated\n",
    "    \n",
    "V_mc_reduced = V_mc_reduced / 1000\n",
    "\n",
    "#ridge\n",
    "g_x = reg.predict(create_B(s0))\n",
    "abs_approximation_error_ridge = np.mean(np.abs(g_x - V_mc_reduced))\n",
    "\n",
    "#linear reg\n",
    "abs_approximation_error_lin = np.mean(np.abs(g(create_B(s0), b_poly_reduced)- V_mc_reduced))\n",
    "\n",
    "print(f'With Ridge, the abs approximation error is {abs_approximation_error_ridge}, \\n and with linear regression \\ it is {abs_approximation_error_lin}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validated truncated pseudoinverse and ridge regression regularizations (with the optimal hyperparameters found in previous cross-validations) and found that ridge regression performs superior to the pseudo inverse.\n",
    "\n",
    "When applying ridge and the plain linear regression from a) on a made up test set of 100 data points. We can see that ridge is better at predicting which we can interpret as a better variance bias trade-off. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
